{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the derivative of a function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **derivative** tells us how much a function's output is changing for a given input change. Think back to using $\\frac{rise}{run}$ in algebra class to calculate the slope of a line. Well, what if we had a function that did **not** trace a straight line? Imagine we had such a \"curvy\" function, and wanted to calculate its slope at a specific input point. Well, we would simply use the $\\frac{rise}{run}$ formula and make _run_ infinitesimally small!\n",
    "\n",
    "Here is the **limit definition of the derivative** from calculus, where $f(x)$ is our function of interest and $f'(x)$ its derivative. $\\Delta$ indicates a change in $x$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f'(x) = \\lim_{\\Delta x\\to 0}\\frac{f(x+\\Delta x)-f(x)}{\\Delta x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** $y$ is used interchangeably with $f(x)$ later in the discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's apply this to a real-world example. Say a person sets off walking from an origin point and picks up their pace with each passing second. Let $x$ represent time in seconds and the person's distance from the origin in meters be given by $f(x) = \\frac{1}{3} x^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import useful libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distance function\n",
    "x = np.linspace(0,5,num=100)\n",
    "f = lambda x: x**2/3\n",
    "p = plt.plot(x, f(x))\n",
    "xlab = plt.xlabel('Time (x)', fontsize=14)\n",
    "ylab = plt.ylabel('Distance from Origin (m)', fontsize=14)\n",
    "#plt.savefig('2_distance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How might we find the **instantaneous rate of change** of our walker's position at 3 seconds? Using the power rule from calculus we can calculate the derivative of our function $f(x)$: $$f(x) = \\frac{1}{3} x^2$$  \n",
    "$$f'(x) = \\frac{2}{3} x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging in 3 for $x$ we get $f'(3) = 2$ _meters/second_. We can visualize the walker's unique pace at 3 seconds by adding a **tangent line** to our graph. The slope of the tangent line is equal to the derivative we just calculated at $x = 3$ and it intercepts our original function $f(x)$ **only** at $x = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a line tangent to the distance function at x = 3\n",
    "x = np.linspace(0,5,num=100)\n",
    "f = lambda x: x**2/3\n",
    "p = plt.plot(x, f(x))\n",
    "s = plt.scatter(3, f(3))\n",
    "t = lambda x: 2*x - 3\n",
    "tang = plt.plot(x,t(x),ls='--')\n",
    "plt.ylim(0,8)\n",
    "xlab = plt.xlabel('Time (x)', fontsize=14)\n",
    "ylab = plt.ylabel('Distance from Origin (m)', fontsize=14)\n",
    "#plt.savefig('3_tangent.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does it matter for machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put derivatives in a machine learning context, let's imagine we are modeling the relationship between **x**: bank account balance, and **y**: account holder happiness. The relationship roughly follows a straight line with zero bias. Our machine learning model will have the form $y = w\\cdot x$ and we'll use linear regression to train it to find the best weight $w$ for accurately predicting happiness units. \n",
    "\n",
    "**Aside**: Happiness units are measured from electrodes stuck to the heads of human volunteers. \n",
    "\n",
    "So our starting point is that we have a bunch of real-world examples of account balance ($x$) and corresponding happiness units ($y$) to help us. We'll call this the **training data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training data\n",
    "x = np.linspace(-500,500, num=100)\n",
    "eps = np.random.randn(100)*100\n",
    "f = lambda x: 2*x + eps\n",
    "y = f(x)\n",
    "p = plt.scatter(x,y,edgecolor='k',s=20)\n",
    "titl = plt.title('Training Data', fontsize=14)\n",
    "xlab = plt.xlabel('Account Balance ($)', fontsize=14)\n",
    "ylab = plt.ylabel('Happiness units (hu)', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train our machine learning model, the goal will be to minimize a **cost function** that measures how far off the model's predictions are from reality. For this example we'll use the **quadtratic cost**, also called  **mean squared error (MSE)**: $$C(w) = \\frac{1}{n}\\Sigma_i(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Where $y_i$ is the true happiness level, $\\hat{y}_i$ the predicted happiness level given by $w\\cdot x_i$ and $n$ the number of training examples. The cost function can thus be interpreted as the average squared error of the model, taken across the training data set. The derivative of the above cost function will be the key to optimizing _w_. First though, let's see how the quadratic cost behaves for a few example values of _w_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a function for computing quadratic cost\n",
    "def quad_cost(w, x, y):\n",
    "    '''\n",
    "    Inputs\n",
    "        w: weight (scalar or vector)\n",
    "        x: inputs (vector)\n",
    "        y: outputs (vector)\n",
    "    '''\n",
    "    y_hat = np.dot(w,x.T)\n",
    "    cost = np.mean(np.sum((y-y_hat)**2))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot two example models.\n",
    "# Visualize the residuals for each training data point.\n",
    "# Display quadratic costs.\n",
    "\n",
    "# Change w1 and w2 below to experiment with different values\n",
    "w1 = 5\n",
    "w2 = 1\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(12,6), sharex=True, sharey=True)\n",
    "\n",
    "for k, w in zip([0,1],[w1,w2]):\n",
    "    p = ax[k].scatter(x,y,edgecolor='k',s=20, label='True')\n",
    "    xlab = ax[k].set_xlabel('Account Balance ($)', fontsize=14)\n",
    "    ylab = ax[k].set_ylabel('Happiness units (hu)', fontsize=14)\n",
    "    g1 = lambda x: w*x\n",
    "    ax[k].plot(x,g1(x), label='Predicted')\n",
    "    ax[k].set_title('w = %i' % w, fontsize=14)\n",
    "    ax[k].set_xlim(-550,550)\n",
    "    ax[k].set_ylim(-3000,3000)\n",
    "    ax[k].legend()\n",
    "    for i in range(len(x)):\n",
    "        y2 = g1(x[i])\n",
    "        ax[k].plot((x[i],x[i]),(y[i],y2), linewidth=1, color=sns.color_palette()[1])\n",
    "    c = quad_cost(w,x,y)\n",
    "    t = ax[k].text(-50,-2000,'C(%i) = %.1e $hu^2$' % (w,c), fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lengths of the green lines connecting true values to predicted values represent the inputs $(y_i -\\hat{y}_i)$ to the cost function. Looking at the above figures, you might have guessed that setting $w = 5$ resulted in much greater cost than $w=1$. Note that you can change the values for these weights in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing cost by hill descent (the 1-D precursor to gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the happiness problem we have only one parameter (_w_) to optimize with respect to the cost function. This makes it straightforward to calculate the derivative of the cost function and simply solve for its minimum analytically.  We could even plot quadratic cost as a function of _w_ and pretty much eyeball the solution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot quadratic cost as a function of w\n",
    "weights = np.linspace(1,3,num=1000)\n",
    "errors = [quad_cost(w,x,y) for w in weights]\n",
    "p = plt.plot(weights,errors)\n",
    "xl = plt.xlabel('w', fontsize=14)\n",
    "yl = plt.ylabel('Quadratic Cost', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... but typical machine learning problems have many, many parameters which _all_ affect the cost and _all_ need optimizing. In such cases it is not reasonable to solve for the cost minimum analytically. Nor can you plot the relationship of parameters to cost for a high-dimensional problem.\n",
    "\n",
    "So, for purpose of illustration we will optimize our single-parameter function by a process called **hill descent** which takes advantage of the cost derivative. This concept will serve as the basis of **gradient descent** for solving higher dimensional problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we randomly initialize _w_ to a value of 1.1. If we didn't know the explicit mathematical relationship between _w_ and _C(w)_, the best we could do is approximate the relationship by calculating the cost derivative at $w = 1.1$. Using the power rule and chain rule from calculus, the cost derivative with respect to _w_ is:$$\\frac{dC}{dw} = \\frac{-2}{n}\\Sigma_i (y_i - \\hat{y}_i)\\cdot x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging $w = 1.1$ into this equation (remember, $\\hat{y}_i = w\\cdot x_i$) gives us the **instantaneous rate of change** of $C(w)$ at that point. Since here it is a negative value, we know that _C(w)_ is decreasing as _w_ increases from its starting point of 1.1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a function to calculate the cost derivative\n",
    "def cost_deriv(w,x,y):\n",
    "    '''\n",
    "    Returns the gradient of the quadratic cost function with respect to w\n",
    "    '''\n",
    "    y_hat = np.dot(w,x)\n",
    "    cost_deriv = np.mean(np.sum(2 * -x * (y-y_hat)))\n",
    "    return cost_deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot C(w) and its tanget line at w = 1.1\n",
    "p = plt.plot(weights,errors)\n",
    "xl = plt.xlabel('w', fontsize=14)\n",
    "yl = plt.ylabel('Quadratic Cost', fontsize=14)\n",
    "p = plt.scatter(1.1,quad_cost(1.1,x,y))\n",
    "slope = cost_deriv(1.1,x,y)\n",
    "t = lambda w: slope*(w - 1.1) + quad_cost(1.1,x,y)\n",
    "p = plt.plot(weights,t(weights), ls='--')\n",
    "p = plt.ylim(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with the cost derivative at the current value of _w_, we can iteratively nudge _w_ in the opposite direction until we reach an optimum. Here is some pseudo-code to describe the **update rule for hill descent**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**while** not converged:\n",
    "<p style=\"margin-left: 40px\">$w^{(t+1)}=w^t-\\eta \\frac{dC}{dw}$</p>\n",
    "<p style=\"margin-left: 40px\">$t=t+1$</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...where $t$ is an iteration counter and $\\eta$ the user-defined **learning rate**. The learning rate will be a small, positive number that ensures we move _w_ in small increments and don't overshoot the optimum. Convergence criteria can be defined by a max iteration count, for example, or by a minimum improvement in cost for a given update to _w_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteratively nudge w in the opposite direction of the cost derivative\n",
    "\n",
    "# Change the value of current_w to experiment with different initial values of w\n",
    "current_w = 1.1\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(12,7))\n",
    "for j in range(ax.shape[0]):\n",
    "    for k in range(ax.shape[1]):\n",
    "        # Plot C(w) function\n",
    "        p = ax[j,k].plot(weights,errors)\n",
    "        ax[j,k].scatter(current_w,quad_cost(current_w,x,y))\n",
    "        # Calculate and plot tangent line\n",
    "        slope = cost_deriv(current_w,x,y)\n",
    "        t = lambda w: slope*(w - current_w) + quad_cost(current_w,x,y)\n",
    "        p = ax[j,k].plot(weights,t(weights), ls='--')\n",
    "        # Annotate subplot\n",
    "        ax[j,k].text(1.7,.6e7,'$w = %.3f$' % current_w)\n",
    "        xl = ax[j,k].set_xlabel('w', fontsize=14)\n",
    "        yl = ax[j,k].set_ylabel('Quadratic Cost')\n",
    "        ax[j,k].set_xlim(.9,3.1)\n",
    "        ax[j,k].set_ylim(0,1e7)\n",
    "        # Apply update rule for w\n",
    "        current_w -= (1./23e6)*cost_deriv(current_w,x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we apply our hill descent technique to a higher-dimensional problem? Let's imagine we need to model happiness on three inputs instead of one: \n",
    "1. Account balance\n",
    "2. Cups of coffee consumed\n",
    "3. Local temperature (F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new dataset and display a few data points\n",
    "bal = np.linspace(-500,500, num=100)\n",
    "cof = np.random.randint(0,6,size=100)\n",
    "temp = np.random.normal(loc=80, scale=15, size=100)\n",
    "eps = np.random.randn(100)*100\n",
    "w0, w1, w2, w3 = np.random.randn()*20, np.random.randint(1,4), \\\n",
    "                 np.random.randint(275,301), np.random.randint(-13,-7)\n",
    "hu = w0 + w1*bal + w2*cof + w3*temp + eps\n",
    "df = pd.DataFrame({'balance':bal,'cups_coffee':cof,'temp_F':temp})\n",
    "df['hu'] = hu \n",
    "df = df.sample(frac=1)\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "df.head().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we'll need to optimize a model of the form $$y=w_0+(w_1\\cdot x_0)+(w_2\\cdot x_1)+(w_3\\cdot x_2)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $w_0$ represents the bias, $w_1$ through $w_3$ the variable weights and $x_0$ through $x_2$ the input features. Our cost function is still the same as for 1-D hill descent:$$C(w) = \\frac{1}{n}\\Sigma_i(y_i - \\hat{y}_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also recall the cost derivative formula, since it will inform our calculation of the gradient: $$\\frac{dC}{dw} = \\frac{-2}{n}\\Sigma_i (y_i - \\hat{y}_i)\\cdot x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between hill descent and gradient descent is that now for each round of updates to the weights we'll need to compute the **partial derivative** of each weight with respect to $C(w)$. This means that as we cycle over the model weights to determine their updates, we treat all as constants _except_ for the one we are computing the partial derivative for right now. We store the partial derivatives in a vector called the **gradient** (notation: $\\nabla$), which in this example will be a vector of length 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla C(w) = \\frac{-2}{n} * \\begin{align*}\n",
    "\\Big[\n",
    "&\\Sigma_i(y_i-\\hat{y_i}), \\\\\n",
    "&\\Sigma_i(y_i-\\hat{y_i})\\cdot x_0, \\\\\n",
    "&\\Sigma_i(y_i-\\hat{y_i})\\cdot x_1, \\\\\n",
    "&\\Sigma_i(y_i-\\hat{y_i})\\cdot x_2\n",
    "\\Big]\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that in the pseudo-code below describing the weight updates, the subtraction term $w^t-\\eta \\nabla C(w)$ in fact implies the element-wise subraction of two vectors, each having length 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**while** not converged:\n",
    "<p style=\"margin-left: 40px\">$w^{(t+1)}=w^t-\\eta \\nabla C(w)$</p>\n",
    "<p style=\"margin-left: 40px\">$t=t+1$</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this knowledge to train a multi-dimensional model! First though we need to make a slight modification to the training data. Notice in the equation for $\\nabla C(w)$ that the partial derivative for each weight is calculated by multiplying the error $(y_i-\\hat{y_i})$ by the input feature $x_i$ that the weight interacts with. The bias term $w_0$ doesn't interact with any input feature, but we can say its input feature is **1** so that its partial derivative takes the same form as the others in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant term to the training data\n",
    "df['constant'] = 1\n",
    "df = df[['constant','balance','cups_coffee','temp_F','hu']]\n",
    "df.head().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write functions to calculate partial derivatives and perform gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to compute feature partial derivative\n",
    "def partial_deriv(errors, feature):\n",
    "    derivative = -2 * np.mean(np.sum(errors*feature))\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(features, target, init_weights, step_size, max_iter):\n",
    "    iter_count = 0        # Initialize iteration counter at 0\n",
    "    training_costs = []   # Initialize empty list to store costs during training\n",
    "    converged = False\n",
    "    weights = init_weights\n",
    "    while not converged:\n",
    "        # Compute predictions and errors\n",
    "        predictions = np.dot(features, weights)\n",
    "        errors = target - predictions\n",
    "        # Update weights\n",
    "        for i in range(len(weights)):\n",
    "            deriv = partial_deriv(errors, features[:,i])\n",
    "            weights[i] -= step_size * deriv\n",
    "        # Compute cost and store\n",
    "        current_cost = quad_cost(weights, features, target)\n",
    "        training_costs.append(current_cost)\n",
    "        # Increment the iteration count\n",
    "        iter_count += 1\n",
    "        if iter_count == max_iter:\n",
    "            converged = True\n",
    "    return(weights, training_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset input features and target 'hu' values as numpy arrays\n",
    "features = np.array(df[['constant','balance','cups_coffee','temp_F']]) \n",
    "target = np.array(df['hu'])\n",
    "# Set remaining arguments for gradient_descent() function\n",
    "init_weights = np.zeros(4)\n",
    "step_size = 8e-8\n",
    "max_iter = 10000\n",
    "# Run gradient descent\n",
    "weights, costs = gradient_descent(features, target, init_weights, step_size, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how quadratic cost changed over training iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p = plt.plot(range(max_iter),costs)\n",
    "xlab = plt.xlabel('Iteration', fontsize=14)\n",
    "ylab = plt.ylabel('C(w)', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we generated the multi-dimensional dataset, we set the weights to be picked randomly from within specified ranges. Let's see how our learned weights stack up against the real weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real weights\n",
    "w1, w2, w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learned weights\n",
    "weights[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check how some model predictions stack up against the target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real hu values\n",
    "target[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model predictions\n",
    "np.dot(weights, features[:3,:].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having completed this tutorial, you now have a basic grasp on\n",
    "    1. Derivatives and why they matter to machine learning\n",
    "    2. Implementing hill descent to optimize a single parameter model\n",
    "    3. Implementing gradient descent to optimize higher dimensional models\n",
    "These skills and concepts can serve as your basis both for tackling interesting real-world problems and for learning and understanding more sophisticated machine learning techniques. Best of luck on your journey!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
