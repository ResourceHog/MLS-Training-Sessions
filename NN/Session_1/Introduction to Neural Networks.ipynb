{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Logistic Regression and Activation Function\n",
    "\n",
    "Recall that Linear regression tries to predict the data by <b>finding a linear – straight line – equation</b> to model or predict future data points. It works on <b>continuous numeric variable</b> given a set of inputs. \n",
    "\n",
    "Logistic regression on the other hand tries to find the <b>plane/line that separates the input space into two regions</b> i.e linearly separable. The output is a <b>probability</b> that the given input point belongs to a certain class.\n",
    "<table width=\"600\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/lin_reg.png\" />\n",
    "</td>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/log_reg.png\" />\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Source:  https://simple.wikipedia.org/wiki/Logistic_Regression\n",
    "\n",
    "\n",
    "<table width=\"600\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/sigmoid_vs_linear.png\" alt=\"Biological Neuron and Artificial Neuron\" />\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<ul>\n",
    "<li>The logistic regression formula is <b>derived from the standard linear equation for a straight line</b> that is y=mx + b\n",
    "</li>\n",
    "<li>Using the <b>Sigmoid function (a.k.a Activation Function)</b>, the standard linear formula is transformed to the logistic regression formula\n",
    "</li>\n",
    "<li>This logistic regression function is useful for <b>predicting the class</b> i.e the output has disceret / <b>catagorical values</b>. (versus linear regression where output is a continious variable)\n",
    "</li>\n",
    "<li>The Activation Function therefore <b>adds a non-linearity</b> to the linear function. \n",
    "</li>\n",
    "</ul>\n",
    " \n",
    " #### Common Activation Functions\n",
    "<table width=\"700\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/activation.png\" alt=\"\" />\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biological Neuron vs Artificial Neuron a.k.a Perceptron\n",
    "\n",
    "\n",
    "<table width=\"800\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/neuron_and_artif_neuron.png\" alt=\"Biological Neuron and Artificial Neuron\" />\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<table width=\"400\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/Formula.png\" />\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<ul>\n",
    "  \n",
    "  <li>In the basic neuron model, the dendrites carry the signal to the cell body where they all get summed <b>(weighted Sum)</b>. If the final sum is above a certain threshold, the neuron can fire, sending a spike along its axon</li>\n",
    "  <li>Each neuron performs a <b>dot product</b> of the input and its weights, adds the bias and applies the <b>non-linearity (i.e activation function)</b>, in this case the sigmoid</li>\n",
    "  <li>The idea is that the synaptic strengths (the weights w) are learnable and <b>control the strength of influence</b> (and its direction: excitory (positive weight) or inhibitory (negative weight)) of one neuron on another. \n",
    "</li>\n",
    "<li>The <b>firing rate</b> of the neuron is modeled by with an activation function f, which represents the frequency of the spikes along the axon</li>\n",
    "\n",
    "\n",
    "  \n",
    "</ul>\n",
    "\n",
    "\n",
    "The Artificial Neuron is also called the <b>Sigmoid Neuron</b> when activation function used is Sigmoid. These days however ReLU seems to be the first choice in Deep Neural Network implementations with only the output layer using the Sigmoid activation function\n",
    "\n",
    "<i>Source: http://cs231n.github.io/neural-networks-1/</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAND gate using One Perceptron\n",
    "Another way to represnt the above equation i.e condition for the output, we can rearrange it as follows. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=\"400\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/w.x.png\"/>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<ul>\n",
    "    <li>∑j jwjx  as a dot product, w⋅x≡∑jxjw, where w and x are vectors of weight and input.</li>\n",
    "    <li>Move the threshold to the other side of the inequality, and to replace it by what's known as the perceptron's bias, b≡−threshold</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons can be used is to compute the elementary logical functions i.e underlying computation functions such as AND, OR, and NAND. For example, suppose we have a perceptron with two inputs, each with weight −2, and an overall bias of 3. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=\"400\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/nand.png\"/>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Input 00 produces output 1, since (−2)∗0+(−2)∗0+3=3(−2)∗0+(−2)∗0+3=3 is <b>positive</b>. </li>\n",
    "<li>Inputs 01 and 10 produce output 1. </li>\n",
    "<li>But the input 11 produces output 0, since (−2)∗1+(−2)∗1+3=−1(−2)∗1+(−2)∗1+3=−1 is <b>negative</b>. </li>\n",
    "<li>And so our perceptron implements a <b>NAND gate</b>!</li>\n",
    "</ul>\n",
    "\n",
    "<i>Source: http://neuralnetworksanddeeplearning.com/chap1.html</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorized Form \n",
    "The input data and output lables can be represented in vectorized form. eg, (W.T) . X + b where W and X are the weights and inputs in a vector form. \n",
    "\n",
    "\n",
    "#### Implementation in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#X = [x1,x2] = \n",
    "X = np.array([1,1])\n",
    "#W = [-2,-2]\n",
    "W = np.array([-2,-2])\n",
    "#bias\n",
    "b = 3\n",
    "#output \n",
    "y = ((np.dot(W.T,X) + b) > 0)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network (i.e Multi-Layer Perceptron)\n",
    "\n",
    "A neural network therefore is nothing but a network of such individual perceptrons. Each perceptron is called a node.  The network consists of the <b>input layer, the hidden layer(s) and the output layer</b>. The output of each neuron from layer n is connected to ever other neuron in layer (n+1).  The output layer could have multiple nodes but the output of each output node will always lie between [0,1] i.e probability of belonging to a certain class.\n",
    "\n",
    "<table width=\"600\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/nn.png\" />\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "### Feedforward Network\n",
    "\n",
    "Neural networks where the output from one layer is used as input to the next layer are called feedforward neural networks. This means there are no loops in the network - <b>information is always fed forward, never fed back</b>. \n",
    "\n",
    "<i>Source : https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/</i>\n",
    "\n",
    "### XOR function using multiple perceptrons\n",
    "\n",
    "Tha  XOR implementation with multiple neurons is a simple neural network with input layer of 2, one hidden layer with 2 neurons and 1 output layer\n",
    "\n",
    "<table width=\"800\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/xor.png\" />\n",
    "</td>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/xor_with_perceptrons.png\" />\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "A \"single-layer\" perceptron can't implement XOR. The reason is because the classes in XOR are not linearly separable. You cannot draw a straight line to separate the points (0,0),(1,1) from the points (0,1),(1,0).\n",
    "\n",
    "\n",
    "Source: http://toritris.weebly.com/perceptron-2-logical-operations.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### Feedfoward Implementation of XOR\n",
    "\n",
    "In the feedforward implementation of XOR, the <b>weights and biases have been precomupted</b> to give the right output i.e Zero error. We cannot learn the weights in a iterative manner in a Feedfowar Neural network. \n",
    "\n",
    "[<b>Side Note</b>. In order to LEARN the right weights and biases which would get us to the exact exepcted ouput, we have to feed the input in forrward direction (starting with random weight and bias) , check difference between curent output and expected output,  and then feed info backwards from output to input computing the rate of change of output at each layer with respect to the weights computed in forward direction and then adjusting them to a more optimal value while in the backward direction for each iteration. This step is repeated multiple times for all the inputs.  This iterative step is called Gradient Descent or Backporpagation.]\n",
    "\n",
    "The solution we described to the XOR problem is at a global minimum of the loss function i.e differece between expected output and computed output y-yhat. So gradient descent would have converged to this point.\n",
    "\n",
    "<table width=\"800\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/my_xor_nn_.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x): return x*(x > 0)\n",
    "\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]]) ##input \n",
    "y = np.array([ [0],   [1],   [1],   [0]])  ##expected output\n",
    "\n",
    "Wh = np.array([[1,1],[1,1]]) #weights for hidden layer\n",
    "Wz = np.array([1,-2]) #weights for output (z) layer \n",
    "bias_c = np.array([0,-1]) #bias terms for hidden layer\n",
    "\n",
    "\n",
    "XWh = np.dot(X,Wh.T) + bias_c #output of hidden layer before activation\n",
    "activated_XWh = relu(XWh) #\n",
    "yHat = (np.dot(activated_XWh, Wz.T))\n",
    "Z = relu(yHat)\n",
    "\n",
    "print Z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop Implementation of XOR\n",
    "\n",
    "The code below implements  foward propgation and backward propagation (i.e Graditent Descent) in order to learn the weights that will produce the minimum error i.e the global minimum of the loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01410154]\n",
      " [ 0.99149623]\n",
      " [ 0.991496  ]\n",
      " [ 0.00224726]]\n"
     ]
    }
   ],
   "source": [
    "#   XOR.py-A very simple neural network to do exclusive or.\n",
    "#Source : http://python3.codes/neural-network-python-part-1-sigmoid-function-gradient-descent-backpropagation/\n",
    "\n",
    "import numpy as np\n",
    " \n",
    "epochs = 50000       # Number of iterations\n",
    "inputLayerSize, hiddenLayerSize, outputLayerSize = 2, 3, 1\n",
    " \n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "Y = np.array([ [0],   [1],   [1],   [0]])\n",
    " \n",
    "def sigmoid (x): return 1/(1 + np.exp(-x))      # activation function\n",
    "def sigmoid_der(x): return x * (1 - x)             # derivative of sigmoid\n",
    "                                                # weights on layer inputs\n",
    "Wh = np.random.uniform(size=(inputLayerSize, hiddenLayerSize))\n",
    "Wz = np.random.uniform(size=(hiddenLayerSize,outputLayerSize))\n",
    " \n",
    "for i in range(epochs):\n",
    " \n",
    "    ##forward propagation\n",
    "    H = sigmoid(np.dot(X, Wh))                  # hidden layer results\n",
    "    Z = sigmoid(np.dot(H, Wz))                  # output layer results\n",
    "    #calculate error\n",
    "    E = Y - Z                                   # how much we missed (error)\n",
    "    #backward propatation\n",
    "    dZ = E * sigmoid_der(Z)                        # delta Z\n",
    "    dH = np.dot(dZ,Wz.T) * sigmoid_der(H)             # delta H\n",
    "    #adjust weights for next iteration\n",
    "    Wz +=  H.T.dot(dZ)                          # update output layer weights\n",
    "    Wh +=  X.T.dot(dH)                          # update hidden layer weights\n",
    "     \n",
    "print(Z)                # what have we learnt?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra (Basics) \n",
    "\n",
    "Reference : http://rlhick.people.wm.edu/stories/linear-algebra-python-basics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
