{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biological Neuron vs Artificial Neuron a.k.a Perceptron\n",
    "\n",
    "\n",
    "<table width=\"800\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/neuron_and_artif_neuron.png\" alt=\"Biological Neuron and Artificial Neuron\" />\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<table width=\"400\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/Formula.png\" />\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<ul>\n",
    "  \n",
    "  <li>In the basic neuron model, the dendrites carry the signal to the cell body where they all get summed <b>(weighted Sum)</b>. If the final sum is above a certain threshold, the neuron can fire, sending a spike along its axon</li>\n",
    "  <li>Each neuron performs a <b>dot product</b> of the input and its weights, adds the bias and applies the <b>non-linearity (i.e activation function)</b>, in this case the sigmoid</li>\n",
    "  <li>The idea is that the synaptic strengths (the weights w) are learnable and <b>control the strength of influence</b> (and its direction: excitory (positive weight) or inhibitory (negative weight)) of one neuron on another. \n",
    "</li>\n",
    "<li>The <b>firing rate</b> of the neuron is modeled by with an activation function f, which represents the frequency of the spikes along the axon</li>\n",
    "  \n",
    "</ul>\n",
    "\n",
    "<i>Source: http://cs231n.github.io/neural-networks-1/</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAND gate using One Perceptron\n",
    "\n",
    "Perceptrons can be used is to compute the elementary logical functions i.e underlying computation functions such as AND, OR, and NAND. For example, suppose we have a perceptron with two inputs, each with weight −2, and an overall bias of 3. \n",
    "<table width=\"300\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/nand.png\" />\n",
    "</td>\n",
    "</table>\n",
    "<ul>\n",
    "<li>Input 00 produces output 1, since (−2)∗0+(−2)∗0+3=3(−2)∗0+(−2)∗0+3=3 is <b>positive</b>. </li>\n",
    "<li>Inputs 01 and 10 produce output 1. </li>\n",
    "<li>But the input 11 produces output 0, since (−2)∗1+(−2)∗1+3=−1(−2)∗1+(−2)∗1+3=−1 is <b>negative</b>. </li>\n",
    "<li>And so our perceptron implements a <b>NAND gate</b>!</li>\n",
    "</ul>\n",
    "\n",
    "<i>Source: http://neuralnetworksanddeeplearning.com/chap1.html</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorized Form \n",
    "The input data and output lables can be represented in vectorized form. eg, (W.T) . X + b where W and X are the weights and inputs in a vector form.\n",
    "\n",
    "\n",
    "#### Implementation in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#X = [x1,x2] = \n",
    "X = np.array([1,1])\n",
    "#W = [-2,-2]\n",
    "W = np.array([-2,-2])\n",
    "#bias\n",
    "b = 3\n",
    "#output \n",
    "y = ((np.dot(W.T,X) + b) > 0)\n",
    "\n",
    "print y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Logistic Regression and Activation Function\n",
    "\n",
    "<table width=\"600\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/sigmoid_vs_linear.png\" alt=\"Biological Neuron and Artificial Neuron\" />\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "The “classic” application of logistic regression model is binary classification. \n",
    "\n",
    "However, we can also use “flavors” of logistic to tackle multi-class classification problems. \n",
    "\n",
    "It is useful if we are working with a dataset where the classes are more or less “linearly separable.”\n",
    "\n",
    "\n",
    "<ul>\n",
    "<li>The logistic regression formula is <b>derived from the standard linear equation for a straight line</b> that is y=mx + b\n",
    "</li>\n",
    "<li>Using the <b>Sigmoid function (a.k.a Activation Function)</b>, the standard linear formula is transformed to the logistic regression formula\n",
    "</li>\n",
    "<li>This logistic regression function is useful for <b>predicting the class</b> i.e the output has disceret / <b>catagorical values</b>. (versus linear regression where output is a continious variable)\n",
    "</li>\n",
    "<li>The Activation Function therefore <b>adds a non-linearity</b> to the linear function. \n",
    "</li>\n",
    "</ul>\n",
    " \n",
    " #### Common Activation Functions\n",
    "<table width=\"700\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/activation.png\" alt=\"\" />\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network (i.e Multi-Layer Perceptron)\n",
    "\n",
    "### XOR function using multiple perceptrons\n",
    "\n",
    "Tha  XOR implementation with multiple neurons is a simple neural network with input layer of 2, one hidden layer with 2 neurons and 1 output layer\n",
    "\n",
    "<table width=\"800\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/xor.png\" />\n",
    "</td>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/xor_with_perceptrons.png\" />\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "A \"single-layer\" perceptron can't implement XOR. The reason is because the classes in XOR are not linearly separable. You cannot draw a straight line to separate the points (0,0),(1,1) from the points (0,1),(1,0).\n",
    "\n",
    "Simplify <b>A xor B = (AvB) ^ ¬(A^B)</b>\n",
    "\n",
    "<img src=\"images/XOR_gate.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<ul>\n",
    "<li>The <b>upper of the two red neurons</b> in the first layer has two inputs with synaptic weights of 0.6 each and a threshold of 1, exactly the <b>same as the AND gate</b>.  This is the AND function in the brackets on the right of the formula.  Notice that it is connected to the output <b>neuron with a negative synaptic weight (-2)  accounts for the NOT operator</b> that precedes the brackets on the right hand side.</li>\n",
    "<li>The lower of the two red neurons in the first layer has two synaptic weights of 1.1 and a threshold of 1, just like the OR gate.  This neuron is doing the job of the OR operator in the brackets on the left of the formula.</li>\n",
    "<li>The output neuron is performing another AND operation - the one in the middle of the formula.  Practically, this output neuron is active whenever one of the inputs, A or B is on, but it is overpowered by the inhibition of the upper neuron in cases when both A and B are on.</li>\n",
    "</ul>\n",
    "Source: http://toritris.weebly.com/perceptron-2-logical-operations.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### Feedforward Network\n",
    "\n",
    "Neural networks where the output from one layer is used as input to the next layer are called feedforward neural networks. This means there are no loops in the network - information is always fed forward, never fed back. \n",
    "\n",
    "\n",
    "<i>Source : https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/</i>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra \n",
    "\n",
    "Source : http://rlhick.people.wm.edu/stories/linear-algebra-python-basics.htm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
