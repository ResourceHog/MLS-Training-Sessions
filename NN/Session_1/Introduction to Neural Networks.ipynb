{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biological Neuron vs Artificial Neuron a.k.a Perceptron\n",
    "\n",
    "\n",
    "<table width=\"800\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/neuron_and_artif_neuron.png\" alt=\"Biological Neuron and Artificial Neuron\" />\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<table width=\"400\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/Formula.png\" />\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<ul>\n",
    "  \n",
    "  <li>In the basic neuron model, the dendrites carry the signal to the cell body where they all get summed <b>(weighted Sum)</b>. If the final sum is above a certain threshold, the neuron can fire, sending a spike along its axon</li>\n",
    "  <li>Each neuron performs a <b>dot product</b> of the input and its weights, adds the bias and applies the <b>non-linearity (i.e activation function)</b>, in this case the sigmoid</li>\n",
    "  <li>The idea is that the synaptic strengths (the weights w) are learnable and <b>control the strength of influence</b> (and its direction: excitory (positive weight) or inhibitory (negative weight)) of one neuron on another. \n",
    "</li>\n",
    "<li>The <b>firing rate</b> of the neuron is modeled by with an activation function f, which represents the frequency of the spikes along the axon</li>\n",
    "  \n",
    "</ul>\n",
    "\n",
    "<i>Source: http://cs231n.github.io/neural-networks-1/</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAND gate using One Perceptron\n",
    "\n",
    "Perceptrons can be used is to compute the elementary logical functions i.e underlying computation functions such as AND, OR, and NAND. For example, suppose we have a perceptron with two inputs, each with weight −2, and an overall bias of 3. \n",
    "\n",
    "<img src=\"images/nand.png\" />\n",
    "\n",
    "<ul>\n",
    "<li>Input 00 produces output 1, since (−2)∗0+(−2)∗0+3=3(−2)∗0+(−2)∗0+3=3 is <b>positive</b>. </li>\n",
    "<li>Inputs 01 and 10 produce output 1. </li>\n",
    "<li>But the input 11 produces output 0, since (−2)∗1+(−2)∗1+3=−1(−2)∗1+(−2)∗1+3=−1 is <b>negative</b>. </li>\n",
    "<li>And so our perceptron implements a <b>NAND gate</b>!</li>\n",
    "</ul>\n",
    "\n",
    "<i>Source: http://neuralnetworksanddeeplearning.com/chap1.html</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorized Form \n",
    "The input data and output lables can be represented in vectorized form. eg, (W.T) . X + b where W and X are the weights and inputs in a vector form. \n",
    "\n",
    "\n",
    "#### Implementation in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-1-824e97168206>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-824e97168206>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    print y\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#X = [x1,x2] = \n",
    "X = np.array([1,1])\n",
    "#W = [-2,-2]\n",
    "W = np.array([-2,-2])\n",
    "#bias\n",
    "b = 3\n",
    "#output \n",
    "y = ((np.dot(W.T,X) + b) > 0)\n",
    "\n",
    "print y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Logistic Regression and Activation Function\n",
    "\n",
    "<table width=\"600\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/sigmoid_vs_linear.png\" alt=\"Biological Neuron and Artificial Neuron\" />\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "The “classic” application of logistic regression model is binary classification. \n",
    "\n",
    "However, we can also use “flavors” of logistic to tackle multi-class classification problems. \n",
    "\n",
    "It is useful if we are working with a dataset where the classes are more or less “linearly separable.”\n",
    "\n",
    "\n",
    "<ul>\n",
    "<li>The logistic regression formula is <b>derived from the standard linear equation for a straight line</b> that is y=mx + b\n",
    "</li>\n",
    "<li>Using the <b>Sigmoid function (a.k.a Activation Function)</b>, the standard linear formula is transformed to the logistic regression formula\n",
    "</li>\n",
    "<li>This logistic regression function is useful for <b>predicting the class</b> i.e the output has disceret / <b>catagorical values</b>. (versus linear regression where output is a continious variable)\n",
    "</li>\n",
    "<li>The Activation Function therefore <b>adds a non-linearity</b> to the linear function. \n",
    "</li>\n",
    "</ul>\n",
    " \n",
    " #### Common Activation Functions\n",
    "<table width=\"700\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/activation.png\" alt=\"\" />\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network (i.e Multi-Layer Perceptron)\n",
    "\n",
    "\n",
    "### Feedforward Network\n",
    "\n",
    "Neural networks where the output from one layer is used as input to the next layer are called feedforward neural networks. This means there are no loops in the network - information is always fed forward, never fed back. \n",
    "\n",
    "<i>Source : https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/</i>\n",
    "\n",
    "### XOR function using multiple perceptrons\n",
    "\n",
    "Tha  XOR implementation with multiple neurons is a simple neural network with input layer of 2, one hidden layer with 2 neurons and 1 output layer\n",
    "\n",
    "<table width=\"800\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/xor.png\" />\n",
    "</td>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/xor_with_perceptrons.png\" />\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "A \"single-layer\" perceptron can't implement XOR. The reason is because the classes in XOR are not linearly separable. You cannot draw a straight line to separate the points (0,0),(1,1) from the points (0,1),(1,0).\n",
    "\n",
    "\n",
    "Source: http://toritris.weebly.com/perceptron-2-logical-operations.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### Feedfoward Implementation of XOR\n",
    "\n",
    "In a feedforward implementation of XOR, the weights and biases have been precomupted to give the right output i.e Zero error. The solution we described to the XOR problem is at a global minimum of the loss function. A gradient-based optimization algorithm can ﬁnd parameters that produce very little error. So gradient descent would have converged to this point.\n",
    "\n",
    "<table width=\"800\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/my_xor_nn_.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x): return x*(x > 0)\n",
    "\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]]) ##input \n",
    "y = np.array([ [0],   [1],   [1],   [0]])  ##expected output\n",
    "\n",
    "Wh = np.array([[1,1],[1,1]]) #weights for hidden layer\n",
    "Wz = np.array([1,-2]) #weights for output (z) layer \n",
    "bias_c = np.array([0,-1]) #bias terms for hidden layer\n",
    "\n",
    "\n",
    "XWh = np.dot(X,Wh.T) + bias_c #output of hidden layer before activation\n",
    "activated_XWh = relu(XWh) #\n",
    "yHat = (np.dot(activated_XWh, Wz.T))\n",
    "Z = relu(yHat)\n",
    "\n",
    "print (Z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Feedforward network\n",
    "\n",
    "This shows a neural network with one input layer, two hidden layers and one ouput layer.\n",
    "One hidden layer has 4 elements and other hidden layer has 3 hidden elements.\n",
    "\n",
    "<table width=\"800\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/multilayerff2.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<table width=\"800\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/activation1.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<table width=\"800\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/activation4.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Source: https://www.coursera.org/learn/neural-networks-deep-learning/lecture/tyAGh/computing-a-neural-networks-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "Cost function is usually calculated after running one epoch (single pass of entire training set). Cost function is used to calculate the gradients which is fundamental of back propagation. Cost function depends on the weights and biases.\n",
    "\n",
    "\n",
    "<table width=\"800\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/error.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Log loss graph\n",
    "\n",
    "<table width=\"800\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/logistic_loss.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Source :http://www.kaiyin.co.vu/2014/04/logistic-regression-with-gradient_7.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop Implementation of XOR\n",
    "\n",
    "The code below implements  foward propgation and backward propagation (i.e Graditent Descent) in order to learn the weights that will produce the minimum error i.e the global minimum of the loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01413497]\n",
      " [ 0.991483  ]\n",
      " [ 0.99148342]\n",
      " [ 0.00220432]]\n"
     ]
    }
   ],
   "source": [
    "#   XOR.py-A very simple neural network to do exclusive or.\n",
    "#Source : http://python3.codes/neural-network-python-part-1-sigmoid-function-gradient-descent-backpropagation/\n",
    "\n",
    "import numpy as np\n",
    " \n",
    "epochs = 50000       # Number of iterations\n",
    "inputLayerSize, hiddenLayerSize, outputLayerSize = 2, 3, 1\n",
    " \n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "Y = np.array([ [0],   [1],   [1],   [0]])\n",
    " \n",
    "def sigmoid (x): return 1/(1 + np.exp(-x))      # activation function\n",
    "def sigmoid_der(x): return x * (1 - x)             # derivative of sigmoid\n",
    "                                                # weights on layer inputs\n",
    "Wh = np.random.uniform(size=(inputLayerSize, hiddenLayerSize))\n",
    "Wz = np.random.uniform(size=(hiddenLayerSize,outputLayerSize))\n",
    " \n",
    "for i in range(epochs):\n",
    " \n",
    "    ##forward propagation\n",
    "    H = sigmoid(np.dot(X, Wh))                  # hidden layer results\n",
    "    Z = sigmoid(np.dot(H, Wz))                  # output layer results\n",
    "    #calculate error\n",
    "    E = Y - Z                                   # how much we missed (error)\n",
    "    #backward propatation\n",
    "    dZ = E * sigmoid_der(Z)                        # delta Z\n",
    "    dH = np.dot(dZ,Wz.T) * sigmoid_der(H)             # delta H\n",
    "    #adjust weights for next iteration\n",
    "    Wz +=  H.T.dot(dZ)                          # update output layer weights\n",
    "    Wh +=  X.T.dot(dH)                          # update hidden layer weights\n",
    "     \n",
    "print(Z)                # what have we learnt?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra (Basics) \n",
    "\n",
    "Reference : http://rlhick.people.wm.edu/stories/linear-algebra-python-basics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
