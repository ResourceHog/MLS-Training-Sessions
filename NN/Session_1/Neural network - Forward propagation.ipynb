{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer(Deep) neural network\n",
    "\n",
    "Logistic regression can be viewed as a shallow network as it has only a single layer between input and output layers. Layer between Input and output layers is called Hidden layer. Neural networks in general have multiple hidden layers. The deeper the network (more hidden layers), network will be able to learn more complex features.\n",
    "\n",
    "In neural networks, there are two main concepts - Forward propagation and backward propagation.Forward propagation uses the parameters (weights and biases) at every layer to calculate and  predict the output. When you start the neural network, you start with random weights and biases for every layer. Back propagation is the mechanism by which you can correct the weights and biases in every layer to improve your estimate of the output.\n",
    "\n",
    "While running any ML algorithm, the data set is split into atleast three sets - Training set, validation set and test set. Training set is used for parameter estimation.Validation set is used for determining better performing and generalizing parameters and architecture. Test set is used for final evaluation of the algorithm.\n",
    "\n",
    "The figure shows a 3 layer neural network with one input layer, two hidden layers and one output layer. Input layer is not considered as a layer as it does not do any operations on the values other than passing it.\n",
    "\n",
    "One hidden layer has 4 elements and other hidden layer has 3 hidden elements. \n",
    "\n",
    "\n",
    "<table width=\"800\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/activation3.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<table width=\"800\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/activation1.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<table width=\"800\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/activation4.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Source: https://www.coursera.org/learn/neural-networks-deep-learning/lecture/tyAGh/computing-a-neural-networks-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "Loss(Error) function is the measure of how well our network performs. The lower the Loss function, our prediction is close to the original output. Cost function is just an aggregated error function value over all training samples. Cost function is usually calculated after running one epoch (single pass of entire training set). Cost function is used for calculating the gradients which is fundamental for back propagation. Cost function depends on the weights and biases. So adjusting the weights and biases for the same input will lead to different cost function values.\n",
    "\n",
    "\n",
    "<table width=\"800\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/error.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "### Log loss graph\n",
    "\n",
    "You can observe how the loss goes to zero when the predicted output and true output are the same.\n",
    "\n",
    "<table width=\"800\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/logistic_loss.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "### Backpropogation intuition\n",
    "\n",
    "we know the cost and knobs (weights and biases) of the algorithm. So when the cost is high , you want to adjust the parameters such that the cost reduces. The change in cost with respect to weights and biases (gradient values) is what we need to calculate. In the error space, you want to go in the opposite direction of the gradient to reach the minimum error value. \n",
    "\n",
    "Source :http://www.kaiyin.co.vu/2014/04/logistic-regression-with-gradient_7.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
